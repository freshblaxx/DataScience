{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import rcParams, style\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from warnings import simplefilter\n",
    "from cycler import cycler\n",
    "\n",
    "# _________________________________________ STYLE ____________________________________________________\n",
    "simplefilter(\"ignore\")\n",
    "\n",
    "HATCHES = [\".\", \"..\", \"...\", \"o\"]  # ['/', '+', 'X', '*'] #'oo', 'OO', '..'\n",
    "\n",
    "my_palette = {\n",
    "    \"yellow\": \"#ECD474\",\n",
    "    \"pale orange\": \"#E9AE4E\",\n",
    "    \"salmon\": \"#E2A36B\",\n",
    "    \"orange\": \"#F79522\",\n",
    "    \"dark orange\": \"#D7725E\",\n",
    "    \"pale acqua\": \"#92C4AF\",\n",
    "    \"acqua\": \"#64B29E\",\n",
    "    \"marine\": \"#3D9EA9\",\n",
    "    \"green\": \"#10A48A\",\n",
    "    \"olive\": \"#99C244\",\n",
    "    \"pale blue\": \"#BDDDE0\",\n",
    "    \"blue2\": \"#199ED5\",\n",
    "    \"blue3\": \"#1DAFE5\",\n",
    "    \"dark blue\": \"#0C70B2\",\n",
    "    \"pale pink\": \"#D077AC\",\n",
    "    \"pink\": \"#EA4799\",\n",
    "    \"lavender\": \"#E09FD5\",\n",
    "    \"lilac\": \"#B081B9\",\n",
    "    \"purple\": \"#923E97\",\n",
    "    \"white\": \"#FFFFFF\",\n",
    "    \"light grey\": \"#D2D3D4\",\n",
    "    \"grey\": \"#939598\",\n",
    "    \"black\": \"#000000\",\n",
    "    \"red\": \"#FF0000\"\n",
    "}\n",
    "\n",
    "colors_pale = [my_palette[\"salmon\"], my_palette[\"blue2\"], my_palette[\"acqua\"]]\n",
    "colors_soft = [my_palette[\"dark orange\"], my_palette[\"dark blue\"], my_palette[\"green\"]]\n",
    "colors_live = [my_palette[\"orange\"], my_palette[\"blue3\"], my_palette[\"olive\"]]\n",
    "blues = [\n",
    "    my_palette[\"pale blue\"],\n",
    "    my_palette[\"blue2\"],\n",
    "    my_palette[\"blue3\"],\n",
    "    my_palette[\"dark blue\"],\n",
    "]\n",
    "oranges = [\n",
    "    my_palette[\"pale orange\"],\n",
    "    my_palette[\"salmon\"],\n",
    "    my_palette[\"orange\"],\n",
    "    my_palette[\"dark orange\"],\n",
    "]\n",
    "ACTIVE_COLORS = [\n",
    "    my_palette[\"blue2\"],\n",
    "    my_palette[\"pink\"],\n",
    "    my_palette[\"yellow\"],\n",
    "    my_palette[\"pale orange\"],\n",
    "    my_palette[\"acqua\"],\n",
    "    my_palette[\"lavender\"],\n",
    "    my_palette[\"salmon\"],\n",
    "    my_palette[\"green\"],\n",
    "    my_palette[\"pale pink\"],\n",
    "    my_palette[\"lilac\"],\n",
    "    my_palette[\"purple\"],\n",
    "\n",
    "]\n",
    "\n",
    "cmap_orange = LinearSegmentedColormap.from_list(\"myCMPOrange\", oranges)\n",
    "cmap_blues = LinearSegmentedColormap.from_list(\"myCMPBlues\", blues)\n",
    "cmap_active = LinearSegmentedColormap.from_list(\"myCMPActive\", ACTIVE_COLORS)\n",
    "\n",
    "LINE_COLOR = my_palette[\"dark blue\"]\n",
    "FILL_COLOR = my_palette[\"blue2\"]  # my_palette[\"pale blue\"]\n",
    "DOT_COLOR = my_palette[\"blue3\"]\n",
    "\n",
    "PAST_COLOR = FILL_COLOR\n",
    "FUTURE_COLOR = my_palette['pale pink']\n",
    "PRED_PAST_COLOR = my_palette['yellow']\n",
    "PRED_FUTURE_COLOR = my_palette['red']\n",
    "\n",
    "rcParams[\"axes.prop_cycle\"] = cycler(\"color\", ACTIVE_COLORS)\n",
    "\n",
    "rcParams[\"text.color\"] = LINE_COLOR\n",
    "rcParams[\"patch.edgecolor\"] = LINE_COLOR\n",
    "rcParams[\"patch.facecolor\"] = FILL_COLOR\n",
    "rcParams[\"axes.facecolor\"] = my_palette[\"white\"]\n",
    "rcParams[\"axes.edgecolor\"] = my_palette[\"grey\"]\n",
    "rcParams[\"axes.labelcolor\"] = my_palette[\"grey\"]\n",
    "rcParams[\"xtick.color\"] = my_palette[\"grey\"]\n",
    "rcParams[\"ytick.color\"] = my_palette[\"grey\"]\n",
    "\n",
    "rcParams[\"grid.color\"] = my_palette[\"light grey\"]\n",
    "\n",
    "rcParams[\"boxplot.boxprops.color\"] = FILL_COLOR\n",
    "rcParams[\"boxplot.capprops.color\"] = LINE_COLOR\n",
    "rcParams[\"boxplot.flierprops.color\"] = my_palette[\"pink\"]\n",
    "rcParams[\"boxplot.flierprops.markeredgecolor\"] = FILL_COLOR\n",
    "rcParams[\"boxplot.flierprops.markerfacecolor\"] = FILL_COLOR\n",
    "rcParams[\"boxplot.whiskerprops.color\"] = LINE_COLOR\n",
    "rcParams[\"boxplot.meanprops.color\"] = my_palette[\"purple\"]\n",
    "rcParams[\"boxplot.meanprops.markeredgecolor\"] = my_palette[\"purple\"]\n",
    "rcParams[\"boxplot.meanprops.markerfacecolor\"] = my_palette[\"purple\"]\n",
    "rcParams[\"boxplot.medianprops.color\"] = my_palette[\"green\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from math import pi, sin, cos, ceil, sqrt\n",
    "from itertools import product\n",
    "from datetime import datetime\n",
    "from typing import Callable\n",
    "from numpy import array, ndarray, arange, std, set_printoptions\n",
    "from matplotlib.collections import PathCollection\n",
    "from matplotlib.colorbar import Colorbar\n",
    "from matplotlib.container import BarContainer\n",
    "from matplotlib.figure import Figure\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from matplotlib.axes import Axes\n",
    "from matplotlib.pyplot import gca, gcf, savefig, subplots, text\n",
    "from matplotlib.dates import AutoDateLocator, AutoDateFormatter\n",
    "\n",
    "# from matplotlib.dates import _reset_epoch_test_example, set_epoch\n",
    "from pandas import DataFrame, Series, Index, Period\n",
    "from pandas import read_csv, concat, to_numeric, to_datetime\n",
    "from pandas.api.types import is_integer_dtype, is_any_real_numeric_dtype\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix, RocCurveDisplay, roc_auc_score\n",
    "from sklearn.naive_bayes import _BaseNB, GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "NR_COLUMNS: int = 3\n",
    "HEIGHT: int = 4\n",
    "\n",
    "TEXT_MARGIN = 0.05\n",
    "FONT_SIZE = 6\n",
    "FONT_TEXT = FontProperties(size=FONT_SIZE)\n",
    "\n",
    "alpha = 0.3\n",
    "\n",
    "NR_STDEV: int = 2\n",
    "IQR_FACTOR: float = 1.5\n",
    "\n",
    "# _reset_epoch_test_example()\n",
    "# set_epoch('0000-12-31T00:00:00')  # old epoch (pre MPL 3.3)\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "#             DATA CHARTS\n",
    "# ---------------------------------------\n",
    "\n",
    "\n",
    "def define_grid(nr_vars, vars_per_row: int = NR_COLUMNS) -> tuple[int, int]:\n",
    "    nr_rows: int = 1\n",
    "    if nr_vars % vars_per_row == 0:\n",
    "        nr_rows = nr_vars // vars_per_row\n",
    "    else:\n",
    "        nr_rows = nr_vars // vars_per_row + 1\n",
    "    return nr_rows, vars_per_row\n",
    "\n",
    "\n",
    "def set_chart_labels(ax: Axes, title: str = \"\", xlabel: str = \"\", ylabel: str = \"\") -> Axes:\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    return ax\n",
    "\n",
    "\n",
    "def set_chart_xticks(xvalues: list[str | int | float | datetime], ax: Axes, percentage: bool = False) -> Axes:\n",
    "    if len(xvalues) > 0:\n",
    "        if percentage:\n",
    "            ax.set_ylim(0.0, 1.0)\n",
    "\n",
    "        if isinstance(xvalues[0], datetime):\n",
    "            locator = AutoDateLocator()\n",
    "            ax.xaxis.set_major_locator(locator)\n",
    "            ax.xaxis.set_major_formatter(AutoDateFormatter(locator, defaultfmt=\"%Y-%m-%d\"))\n",
    "        rotation: int = 0\n",
    "        if not any(not isinstance(x, (int, float)) for x in xvalues):\n",
    "            ax.set_xlim(left=xvalues[0], right=xvalues[-1])\n",
    "            ax.set_xticks(xvalues, labels=xvalues)\n",
    "        else:\n",
    "            rotation = 45\n",
    "\n",
    "        ax.tick_params(axis=\"x\", labelrotation=rotation, labelsize=\"xx-small\")\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_line_chart(\n",
    "    xvalues: list,\n",
    "    yvalues: list,\n",
    "    ax: Axes = None,  # type: ignore\n",
    "    title: str = \"\",\n",
    "    xlabel: str = \"\",\n",
    "    ylabel: str = \"\",\n",
    "    name: str = \"\",\n",
    "    percentage: bool = False,\n",
    "    show_stdev: bool = False,\n",
    ") -> Axes:\n",
    "    if ax is None:\n",
    "        ax = gca()\n",
    "    ax = set_chart_labels(ax=ax, title=title, xlabel=xlabel, ylabel=ylabel)\n",
    "    ax = set_chart_xticks(xvalues, ax, percentage=percentage)\n",
    "    if any(y < 0 for y in yvalues) and percentage:\n",
    "            ax.set_ylim(-1.0, 1.0)\n",
    "    ax.plot(xvalues, yvalues, c=LINE_COLOR, label=name)\n",
    "    if show_stdev:\n",
    "        stdev: float = round(std(yvalues), 3)\n",
    "        y_bottom: list[float] = [(y - stdev) for y in yvalues]\n",
    "        y_top: list[float] = [(y + stdev) for y in yvalues]\n",
    "        ax.fill_between(xvalues, y_bottom, y_top, color=FILL_COLOR, alpha=0.2)\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_bar_chart(\n",
    "    xvalues: list,\n",
    "    yvalues: list,\n",
    "    ax: Axes = None,  # type: ignore\n",
    "    title: str = \"\",\n",
    "    xlabel: str = \"\",\n",
    "    ylabel: str = \"\",\n",
    "    percentage: bool = False,\n",
    ") -> Axes:\n",
    "    if ax is None:\n",
    "        ax = gca()\n",
    "    ax = set_chart_labels(ax=ax, title=title, xlabel=xlabel, ylabel=ylabel)\n",
    "    ax = set_chart_xticks(xvalues, ax=ax, percentage=percentage)\n",
    "    values: BarContainer = ax.bar(\n",
    "        xvalues,\n",
    "        yvalues,\n",
    "        label=yvalues,\n",
    "        edgecolor=LINE_COLOR,\n",
    "        color=FILL_COLOR,\n",
    "        tick_label=xvalues,\n",
    "    )\n",
    "    format = \"%.2f\" if percentage else \"%.0f\"\n",
    "    ax.bar_label(values, fmt=format, fontproperties=FONT_TEXT)\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_scatter_chart(\n",
    "    var1: list,\n",
    "    var2: list,\n",
    "    ax: Axes = None,  # type: ignore\n",
    "    title: str = \"\",\n",
    "    xlabel: str = \"\",\n",
    "    ylabel: str = \"\",\n",
    ") -> Axes:\n",
    "    if ax is None:\n",
    "        ax = gca()\n",
    "    ax = set_chart_labels(ax=ax, title=title, xlabel=xlabel, ylabel=ylabel)\n",
    "    ax.scatter(var1, var2)\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_horizontal_bar_chart(\n",
    "    elements: list,\n",
    "    values: list,\n",
    "    error: list = [],\n",
    "    ax: Axes = None,  # type: ignore\n",
    "    title: str = \"\",\n",
    "    xlabel: str = \"\",\n",
    "    ylabel: str = \"\",\n",
    "    percentage: bool = False,\n",
    ") -> Axes:\n",
    "    if ax is None:\n",
    "        ax = gca()\n",
    "    if percentage:\n",
    "        ax.set_xlim((0, 1))\n",
    "    if error == []:\n",
    "        error = [0] * len(elements)\n",
    "    ax = set_chart_labels(ax=ax, title=title, xlabel=xlabel, ylabel=ylabel)\n",
    "    y_pos: list = list(arange(len(elements)))\n",
    "\n",
    "    ax.barh(y_pos, values, xerr=error, align=\"center\", error_kw={\"lw\": 0.5, \"ecolor\": \"r\"})\n",
    "    ax.set_yticks(y_pos, labels=elements)\n",
    "    ax.invert_yaxis()  # labels read top-to-bottom\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_multiline_chart(\n",
    "    xvalues: list,\n",
    "    yvalues: dict,\n",
    "    ax: Axes = None,  # type: ignore\n",
    "    title: str = \"\",\n",
    "    xlabel: str = \"\",\n",
    "    ylabel: str = \"\",\n",
    "    percentage: bool = False,\n",
    ") -> Axes:\n",
    "    if ax is None:\n",
    "        ax = gca()\n",
    "    ax = set_chart_labels(ax=ax, title=title, xlabel=xlabel, ylabel=ylabel)\n",
    "    ax = set_chart_xticks(xvalues, ax=ax, percentage=percentage)\n",
    "    legend: list = []\n",
    "    for name, y in yvalues.items():\n",
    "        ax.plot(xvalues, y)\n",
    "        legend.append(name)\n",
    "        if any(v < 0 for v in y) and percentage:\n",
    "            ax.set_ylim(-1.0, 1.0)\n",
    "    ax.legend(legend, fontsize=\"xx-small\")\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_multibar_chart(\n",
    "    group_labels: list,\n",
    "    yvalues: dict,\n",
    "    ax: Axes = None,  # type: ignore\n",
    "    title: str = \"\",\n",
    "    xlabel: str = \"\",\n",
    "    ylabel: str = \"\",\n",
    "    percentage: bool = False,\n",
    ") -> Axes | list[Axes]:\n",
    "    if ax is None:\n",
    "        ax = gca()\n",
    "    ax = set_chart_labels(ax=ax, title=title, xlabel=xlabel, ylabel=ylabel)\n",
    "    if percentage:\n",
    "        ax.set_ylim(0.0, 1.0)\n",
    "    bar_labels: list = list(yvalues.keys())\n",
    "\n",
    "    # This is the location for each bar\n",
    "    index: ndarray = arange(len(group_labels))\n",
    "    bar_width: float = 0.8 / len(bar_labels)\n",
    "    ax.set_xticks(index + bar_width / 2, labels=group_labels)\n",
    "\n",
    "    for i in range(len(bar_labels)):\n",
    "        bar_yvalues = yvalues[bar_labels[i]]\n",
    "        values: BarContainer = ax.bar(\n",
    "            index + i * bar_width,\n",
    "            bar_yvalues,\n",
    "            width=bar_width,\n",
    "            label=bar_labels[i],\n",
    "        )\n",
    "        format = \"%.2f\" if percentage else \"%.0f\"\n",
    "        ax.bar_label(values, fmt=format, fontproperties=FONT_TEXT)\n",
    "        if any(y < 0 for y in bar_yvalues) and percentage:\n",
    "            ax.set_ylim(-1.0, 1.0)\n",
    "    ax.legend(fontsize=\"xx-small\")\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_multi_scatters_chart(\n",
    "    data: DataFrame, var1: str, var2: str, var3: str = \"\", ax: Axes = None  # type: ignore\n",
    ") -> Axes:\n",
    "    if ax is None:\n",
    "        ax = gca()\n",
    "\n",
    "    title: str = f\"{var1} x {var2}\"\n",
    "    if var3 != \"\":\n",
    "        title += f\"per {var3}\"\n",
    "        if is_any_real_numeric_dtype(data[var3]) and not is_integer_dtype(data[var3]):\n",
    "            chart: PathCollection = ax.scatter(data[var1], data[var2], c=data[var3].to_list())\n",
    "            cbar: Colorbar = gcf().colorbar(chart)\n",
    "            cbar.outline.set_visible(False)  # type: ignore\n",
    "            cbar.set_label(var3, loc=\"top\")\n",
    "        else:\n",
    "            values: list = data[var3].unique().tolist()\n",
    "            values.sort()\n",
    "            for i in range(len(values)):\n",
    "                subset: DataFrame = data[data[var3] == values[i]]\n",
    "                ax.scatter(subset[var1], subset[var2], color=ACTIVE_COLORS[i], label=values[i])\n",
    "            ax.legend(fontsize=\"xx-small\")\n",
    "    else:\n",
    "        ax.scatter(data[var1], data[var2], color=FILL_COLOR)\n",
    "    ax = set_chart_labels(ax=ax, title=title, xlabel=var1, ylabel=var2)\n",
    "    return ax\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "#             DATA PROFILING\n",
    "# ---------------------------------------\n",
    "\n",
    "\n",
    "def get_variable_types(df: DataFrame) -> dict[str, list]:\n",
    "    variable_types: dict = {\"numeric\": [], \"binary\": [], \"date\": [], \"symbolic\": []}\n",
    "\n",
    "    nr_values: Series = df.nunique(axis=0, dropna=True)\n",
    "    for c in df.columns:\n",
    "        if 2 == nr_values[c]:\n",
    "            variable_types[\"binary\"].append(c)\n",
    "            df[c].astype(\"bool\")\n",
    "        else:\n",
    "            try:\n",
    "                to_numeric(df[c], errors=\"raise\")\n",
    "                variable_types[\"numeric\"].append(c)\n",
    "            except ValueError:\n",
    "                try:\n",
    "                    df[c] = to_datetime(df[c], errors=\"raise\")\n",
    "                    variable_types[\"date\"].append(c)\n",
    "                except ValueError:\n",
    "                    variable_types[\"symbolic\"].append(c)\n",
    "\n",
    "    return variable_types\n",
    "\n",
    "\n",
    "def determine_outlier_thresholds_for_var(\n",
    "    summary5: Series, std_based: bool = True, threshold: float = NR_STDEV\n",
    ") -> tuple[float, float]:\n",
    "    top: float = 0\n",
    "    bottom: float = 0\n",
    "    if std_based:\n",
    "        std: float = threshold * summary5[\"std\"]\n",
    "        top = summary5[\"mean\"] + std\n",
    "        bottom = summary5[\"mean\"] - std\n",
    "    else:\n",
    "        iqr: float = threshold * (summary5[\"75%\"] - summary5[\"25%\"])\n",
    "        top = summary5[\"75%\"] + iqr\n",
    "        bottom = summary5[\"25%\"] - iqr\n",
    "\n",
    "    return top, bottom\n",
    "\n",
    "\n",
    "def count_outliers(\n",
    "    data: DataFrame,\n",
    "    numeric: list[str],\n",
    "    nrstdev: int = NR_STDEV,\n",
    "    iqrfactor: float = IQR_FACTOR,\n",
    ") -> dict:\n",
    "    outliers_iqr: list = []\n",
    "    outliers_stdev: list = []\n",
    "    summary5: DataFrame = data[numeric].describe()\n",
    "\n",
    "    for var in numeric:\n",
    "        top: float\n",
    "        bottom: float\n",
    "        top, bottom = determine_outlier_thresholds_for_var(summary5[var], std_based=True, threshold=nrstdev)\n",
    "        outliers_stdev += [data[data[var] > top].count()[var] + data[data[var] < bottom].count()[var]]\n",
    "\n",
    "        top, bottom = determine_outlier_thresholds_for_var(summary5[var], std_based=False, threshold=iqrfactor)\n",
    "        outliers_iqr += [data[data[var] > top].count()[var] + data[data[var] < bottom].count()[var]]\n",
    "\n",
    "    return {\"iqr\": outliers_iqr, \"stdev\": outliers_stdev}\n",
    "\n",
    "\n",
    "def derive_date_variables(df: DataFrame, date_vars: list[str]) -> DataFrame:\n",
    "    for date in date_vars:\n",
    "        df[date + \"_year\"] = df[date].dt.year\n",
    "        df[date + \"_quarter\"] = df[date].dt.quarter\n",
    "        df[date + \"_month\"] = df[date].dt.month\n",
    "        df[date + \"_day\"] = df[date].dt.day\n",
    "    return df\n",
    "\n",
    "\n",
    "def analyse_date_granularity(data: DataFrame, var: str, levels: list[str]) -> ndarray:\n",
    "    cols: int = len(levels)\n",
    "    fig: Figure\n",
    "    axs: ndarray\n",
    "    fig, axs = subplots(1, cols, figsize=(cols * HEIGHT, HEIGHT), squeeze=False)\n",
    "    fig.suptitle(f\"Granularity study for {var}\")\n",
    "\n",
    "    for i in range(cols):\n",
    "        counts: Series[int] = data[var + \"_\" + levels[i]].value_counts()\n",
    "        plot_bar_chart(\n",
    "            counts.index.to_list(),\n",
    "            counts.to_list(),\n",
    "            ax=axs[0, i],\n",
    "            title=levels[i],\n",
    "            xlabel=levels[i],\n",
    "            ylabel=\"nr records\",\n",
    "            percentage=False,\n",
    "        )\n",
    "    return axs\n",
    "\n",
    "\n",
    "def analyse_property_granularity(data: DataFrame, property: str, vars: list[str]) -> ndarray:\n",
    "    cols: int = len(vars)\n",
    "    fig: Figure\n",
    "    axs: ndarray\n",
    "    fig, axs = subplots(1, cols, figsize=(cols * HEIGHT, HEIGHT), squeeze=False)\n",
    "    fig.suptitle(f\"Granularity study for {property}\")\n",
    "    for i in range(cols):\n",
    "        counts: Series[int] = data[vars[i]].value_counts()\n",
    "        plot_bar_chart(\n",
    "            counts.index.to_list(),\n",
    "            counts.to_list(),\n",
    "            ax=axs[0, i],\n",
    "            title=vars[i],\n",
    "            xlabel=vars[i],\n",
    "            ylabel=\"nr records\",\n",
    "            percentage=False,\n",
    "        )\n",
    "    return axs\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "#             DATA PREPARATION\n",
    "# ---------------------------------------\n",
    "\n",
    "\n",
    "def encode_cyclic_variables(data: DataFrame, vars: list[str]) -> None:\n",
    "    for v in vars:\n",
    "        x_max: float | int = max(data[v])\n",
    "        data[v + \"_sin\"] = data[v].apply(lambda x: round(sin(2 * pi * x / x_max), 3))\n",
    "        data[v + \"_cos\"] = data[v].apply(lambda x: round(cos(2 * pi * x / x_max), 3))\n",
    "    return\n",
    "\n",
    "\n",
    "def dummify(df: DataFrame, vars_to_dummify: list[str]) -> DataFrame:\n",
    "    other_vars: list[str] = [c for c in df.columns if not c in vars_to_dummify]\n",
    "\n",
    "    enc = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False, dtype=\"bool\", drop=\"if_binary\")\n",
    "    trans: ndarray = enc.fit_transform(df[vars_to_dummify])\n",
    "\n",
    "    new_vars: ndarray = enc.get_feature_names_out(vars_to_dummify)\n",
    "    dummy = DataFrame(trans, columns=new_vars, index=df.index)\n",
    "\n",
    "    final_df: DataFrame = concat([df[other_vars], dummy], axis=1)\n",
    "    return final_df\n",
    "\n",
    "\n",
    "def mvi_by_dropping(data: DataFrame, min_pct_per_variable: float = 0.1, min_pct_per_record: float = 0.0) -> DataFrame:\n",
    "    \"\"\"\n",
    "    data: DataFrame - the data to clean\n",
    "    min_pct_per_variable: float - the minimum percentage of records a variable has to show in order to be kept\n",
    "    min_pct_per_record: float - the minimum percentage of values that a record has to show in order to be kept\n",
    "    return the data modified\n",
    "    \"\"\"\n",
    "    # Deleting variables\n",
    "    df: DataFrame = data.dropna(axis=1, thresh=data.shape[0] * min_pct_per_variable, inplace=False)\n",
    "    # Deleting records\n",
    "    df.dropna(axis=0, thresh=data.shape[1] * min_pct_per_record, inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def mvi_by_filling(data: DataFrame, strategy: str = \"frequent\") -> DataFrame:\n",
    "    \"\"\"\n",
    "    data: DataFrame - the data to clean\n",
    "    strategy: str - the strategy to apply ('frequent', 'constant' or 'knn')\n",
    "    return the data modified\n",
    "    \"\"\"\n",
    "    df: DataFrame\n",
    "    variables: dict = get_variable_types(data)\n",
    "    stg_num, v_num = \"mean\", -1\n",
    "    stg_sym, v_sym = \"most_frequent\", \"NA\"\n",
    "    stg_bool, v_bool = \"most_frequent\", False\n",
    "    if strategy != \"knn\":\n",
    "        lst_dfs: list = []\n",
    "        if strategy == \"constant\":\n",
    "            stg_num, stg_sym, stg_bool = \"constant\", \"constant\", \"constant\"\n",
    "        if len(variables[\"numeric\"]) > 0:\n",
    "            imp = SimpleImputer(strategy=stg_num, fill_value=v_num, copy=True)\n",
    "            tmp_nr = DataFrame(\n",
    "                imp.fit_transform(data[variables[\"numeric\"]]),\n",
    "                columns=variables[\"numeric\"],\n",
    "            )\n",
    "            lst_dfs.append(tmp_nr)\n",
    "        if len(variables[\"symbolic\"]) > 0:\n",
    "            imp = SimpleImputer(strategy=stg_sym, fill_value=v_sym, copy=True)\n",
    "            tmp_sb = DataFrame(\n",
    "                imp.fit_transform(data[variables[\"symbolic\"]]),\n",
    "                columns=variables[\"symbolic\"],\n",
    "            )\n",
    "            lst_dfs.append(tmp_sb)\n",
    "        if len(variables[\"binary\"]) > 0:\n",
    "            imp = SimpleImputer(strategy=stg_bool, fill_value=v_bool, copy=True)\n",
    "            tmp_bool = DataFrame(\n",
    "                imp.fit_transform(data[variables[\"binary\"]]),\n",
    "                columns=variables[\"binary\"],\n",
    "            )\n",
    "            lst_dfs.append(tmp_bool)\n",
    "        df = concat(lst_dfs, axis=1)\n",
    "    else:\n",
    "        imp = KNNImputer(n_neighbors=5)\n",
    "        imp.fit(data)\n",
    "        ar: ndarray = imp.transform(data)\n",
    "        df = DataFrame(ar, columns=data.columns, index=data.index)\n",
    "    return df\n",
    "\n",
    "\n",
    "def select_low_variance_variables(data: DataFrame, max_threshold: float, target: str = \"class\") -> list:\n",
    "    summary5: DataFrame = data.describe()\n",
    "    vars2drop: Index[str] = summary5.columns[summary5.loc[\"std\"] * summary5.loc[\"std\"] < max_threshold]\n",
    "    vars2drop = vars2drop.drop(target) if target in vars2drop else vars2drop\n",
    "    return list(vars2drop.values)\n",
    "\n",
    "\n",
    "def study_variance_for_feature_selection(\n",
    "    train: DataFrame,\n",
    "    test: DataFrame,\n",
    "    target: str = \"class\",\n",
    "    max_threshold: float = 1,\n",
    "    lag: float = 0.05,\n",
    "    metric: str = \"accuracy\",\n",
    "    file_tag: str = \"\",\n",
    ") -> dict:\n",
    "    options: list[float] = [round(i * lag, 3) for i in range(1, ceil(max_threshold / lag + lag))]\n",
    "    results: dict[str, list] = {\"NB\": [], \"KNN\": []}\n",
    "    summary5: DataFrame = train.describe()\n",
    "    for thresh in options:\n",
    "        vars2drop: Index[str] = summary5.columns[summary5.loc[\"std\"] * summary5.loc[\"std\"] < thresh]\n",
    "        vars2drop = vars2drop.drop(target) if target in vars2drop else vars2drop\n",
    "\n",
    "        train_copy: DataFrame = train.drop(vars2drop, axis=1, inplace=False)\n",
    "        test_copy: DataFrame = test.drop(vars2drop, axis=1, inplace=False)\n",
    "        eval: dict[str, list] | None = evaluate_approach(train_copy, test_copy, target=target, metric=metric)\n",
    "        if eval is not None:\n",
    "            results[\"NB\"].append(eval[metric][0])\n",
    "            results[\"KNN\"].append(eval[metric][1])\n",
    "\n",
    "    plot_multiline_chart(\n",
    "        options,\n",
    "        results,\n",
    "        title=f\"{file_tag} variance study ({metric})\",\n",
    "        xlabel=\"variance threshold\",\n",
    "        ylabel=metric,\n",
    "        percentage=True,\n",
    "    )\n",
    "    savefig(f\"images/{file_tag}_fs_low_var_{metric}_study.png\")\n",
    "    return results\n",
    "\n",
    "\n",
    "def select_redundant_variables(data: DataFrame, min_threshold: float = 0.90, target: str = \"class\") -> list:\n",
    "    df: DataFrame = data.drop(target, axis=1, inplace=False)\n",
    "    corr_matrix: DataFrame = abs(df.corr())\n",
    "    variables: Index[str] = corr_matrix.columns\n",
    "    vars2drop: list = []\n",
    "    for v1 in variables:\n",
    "        vars_corr: Series = (corr_matrix[v1]).loc[corr_matrix[v1] >= min_threshold]\n",
    "        vars_corr.drop(v1, inplace=True)\n",
    "        if len(vars_corr) > 1:\n",
    "            lst_corr = list(vars_corr.index)\n",
    "            for v2 in lst_corr:\n",
    "                if v2 not in vars2drop:\n",
    "                    vars2drop.append(v2)\n",
    "    return vars2drop\n",
    "\n",
    "\n",
    "def study_redundancy_for_feature_selection(\n",
    "    train: DataFrame,\n",
    "    test: DataFrame,\n",
    "    target: str = \"class\",\n",
    "    min_threshold: float = 0.90,\n",
    "    lag: float = 0.05,\n",
    "    metric: str = \"accuracy\",\n",
    "    file_tag: str = \"\",\n",
    ") -> dict:\n",
    "    options: list[float] = [round(min_threshold + i * lag, 3) for i in range(ceil((1 - min_threshold) / lag) + 1)]\n",
    "\n",
    "    df: DataFrame = train.drop(target, axis=1, inplace=False)\n",
    "    corr_matrix: DataFrame = abs(df.corr())\n",
    "    variables: Index[str] = corr_matrix.columns\n",
    "    results: dict[str, list] = {\"NB\": [], \"KNN\": []}\n",
    "    for thresh in options:\n",
    "        vars2drop: list = []\n",
    "        for v1 in variables:\n",
    "            vars_corr: Series = (corr_matrix[v1]).loc[corr_matrix[v1] >= thresh]\n",
    "            vars_corr.drop(v1, inplace=True)\n",
    "            if len(vars_corr) > 1:\n",
    "                lst_corr = list(vars_corr.index)\n",
    "                for v2 in lst_corr:\n",
    "                    if v2 not in vars2drop:\n",
    "                        vars2drop.append(v2)\n",
    "\n",
    "        train_copy: DataFrame = train.drop(vars2drop, axis=1, inplace=False)\n",
    "        test_copy: DataFrame = test.drop(vars2drop, axis=1, inplace=False)\n",
    "        eval: dict | None = evaluate_approach(train_copy, test_copy, target=target, metric=metric)\n",
    "        if eval is not None:\n",
    "            results[\"NB\"].append(eval[metric][0])\n",
    "            results[\"KNN\"].append(eval[metric][1])\n",
    "\n",
    "    plot_multiline_chart(\n",
    "        options,\n",
    "        results,\n",
    "        title=f\"{file_tag} redundancy study ({metric})\",\n",
    "        xlabel=\"correlation threshold\",\n",
    "        ylabel=metric,\n",
    "        percentage=True,\n",
    "    )\n",
    "    savefig(f\"images/{file_tag}_fs_redundancy_{metric}_study.png\")\n",
    "    return results\n",
    "\n",
    "\n",
    "def apply_feature_selection(\n",
    "    train: DataFrame,\n",
    "    test: DataFrame,\n",
    "    vars2drop: list,\n",
    "    filename: str = \"\",\n",
    "    tag: str = \"\",\n",
    ") -> tuple[DataFrame, DataFrame]:\n",
    "    train_copy: DataFrame = train.drop(vars2drop, axis=1, inplace=False)\n",
    "    train_copy.to_csv(f\"{filename}_train_{tag}.csv\", index=True)\n",
    "    test_copy: DataFrame = test.drop(vars2drop, axis=1, inplace=False)\n",
    "    test_copy.to_csv(f\"{filename}_test_{tag}.csv\", index=True)\n",
    "    return train_copy, test_copy\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "#             CLASSIFICATION\n",
    "# ---------------------------------------\n",
    "\n",
    "\n",
    "DELTA_IMPROVE: float = 0.001\n",
    "\n",
    "\n",
    "CLASS_EVAL_METRICS: dict[str, Callable] = {\n",
    "    \"accuracy\": accuracy_score,\n",
    "    \"recall\": recall_score,\n",
    "    \"precision\": precision_score,\n",
    "    \"auc\": roc_auc_score,\n",
    "    \"f1\": f1_score,\n",
    "}\n",
    "\n",
    "\n",
    "def run_NB(trnX, trnY, tstX, tstY, metric: str = \"accuracy\") -> dict[str, float]:\n",
    "    estimators: dict[str, GaussianNB | MultinomialNB | BernoulliNB] = {\n",
    "        \"GaussianNB\": GaussianNB(),\n",
    "        \"MultinomialNB\": MultinomialNB(),\n",
    "        \"BernoulliNB\": BernoulliNB(),\n",
    "    }\n",
    "    best_model: GaussianNB | MultinomialNB | BernoulliNB = None  # type: ignore\n",
    "    best_performance: float = 0.0\n",
    "    eval: dict[str, float] = {}\n",
    "\n",
    "    for clf in estimators:\n",
    "        estimators[clf].fit(trnX, trnY)\n",
    "        prdY: ndarray = estimators[clf].predict(tstX)\n",
    "        performance: float = CLASS_EVAL_METRICS[metric](tstY, prdY)\n",
    "        if performance - best_performance > DELTA_IMPROVE:\n",
    "            best_performance = performance\n",
    "            best_model = estimators[clf]\n",
    "    if best_model is not None:\n",
    "        prd: ndarray = best_model.predict(tstX)\n",
    "        for key in CLASS_EVAL_METRICS:\n",
    "            eval[key] = CLASS_EVAL_METRICS[key](tstY, prd)\n",
    "    return eval\n",
    "\n",
    "\n",
    "def run_KNN(trnX, trnY, tstX, tstY, metric=\"accuracy\") -> dict[str, float]:\n",
    "    kvalues: list[int] = [1] + [i for i in range(5, 26, 5)]\n",
    "    best_model: KNeighborsClassifier = None  # type: ignore\n",
    "    best_performance: float = 0\n",
    "    eval: dict[str, float] = {}\n",
    "    for k in kvalues:\n",
    "        clf = KNeighborsClassifier(n_neighbors=k, metric=\"euclidean\")\n",
    "        clf.fit(trnX, trnY)\n",
    "        prdY: ndarray = clf.predict(tstX)\n",
    "        performance: float = CLASS_EVAL_METRICS[metric](tstY, prdY)\n",
    "        if performance - best_performance > DELTA_IMPROVE:\n",
    "            best_performance = performance\n",
    "            best_model: KNeighborsClassifier = clf\n",
    "    if best_model is not None:\n",
    "        prd: ndarray = best_model.predict(tstX)\n",
    "        for key in CLASS_EVAL_METRICS:\n",
    "            eval[key] = CLASS_EVAL_METRICS[key](tstY, prd)\n",
    "    return eval\n",
    "\n",
    "\n",
    "def evaluate_approach(\n",
    "    train: DataFrame, test: DataFrame, target: str = \"class\", metric: str = \"accuracy\"\n",
    ") -> dict[str, list]:\n",
    "    trnY = train.pop(target).values\n",
    "    trnX: ndarray = train.values\n",
    "    tstY = test.pop(target).values\n",
    "    tstX: ndarray = test.values\n",
    "    eval: dict[str, list] = {}\n",
    "\n",
    "    eval_NB: dict[str, float] | None = run_NB(trnX, trnY, tstX, tstY, metric=metric)\n",
    "    eval_KNN: dict[str, float] | None = run_KNN(trnX, trnY, tstX, tstY, metric=metric)\n",
    "    if eval_NB != {} and eval_KNN != {}:\n",
    "        for met in CLASS_EVAL_METRICS:\n",
    "            eval[met] = [eval_NB[met], eval_KNN[met]]\n",
    "    return eval\n",
    "\n",
    "\n",
    "def read_train_test_from_files(\n",
    "    train_fn: str, test_fn: str, target: str = \"class\"\n",
    ") -> tuple[ndarray, ndarray, array, array, list, list]:\n",
    "    train: DataFrame = read_csv(train_fn, index_col=None)\n",
    "    labels: list = list(train[target].unique())\n",
    "    labels.sort()\n",
    "    trnY: array = train.pop(target).to_list()\n",
    "    trnX: ndarray = train.values\n",
    "\n",
    "    test: DataFrame = read_csv(test_fn, index_col=None)\n",
    "    tstY: array = test.pop(target).to_list()\n",
    "    tstX: ndarray = test.values\n",
    "    return trnX, tstX, trnY, tstY, labels, train.columns.to_list()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cnf_matrix: ndarray, classes_names: ndarray, ax: Axes = None) -> Axes:  # type: ignore\n",
    "    if ax is None:\n",
    "        ax = gca()\n",
    "    title = \"Confusion matrix\"\n",
    "    set_printoptions(precision=2)\n",
    "    tick_marks: ndarray = arange(0, len(classes_names), 1)\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel(\"True label\")\n",
    "    ax.set_xlabel(\"Predicted label\")\n",
    "    ax.set_xticks(tick_marks)\n",
    "    ax.set_yticks(tick_marks)\n",
    "    ax.set_xticklabels(classes_names)\n",
    "    ax.set_yticklabels(classes_names)\n",
    "    ax.imshow(cnf_matrix, interpolation=\"nearest\", cmap=cmap_blues)\n",
    "\n",
    "    for i, j in product(range(cnf_matrix.shape[0]), range(cnf_matrix.shape[1])):\n",
    "        ax.text(j, i, format(cnf_matrix[i, j], \"d\"), color=\"y\", horizontalalignment=\"center\")\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_roc_chart(tstY: ndarray, predictions: dict, ax: Axes = None, target: str = \"class\") -> Axes:  # type: ignore\n",
    "    if ax is None:\n",
    "        ax = gca()\n",
    "    ax.set_xlim(0.0, 1.0)\n",
    "    ax.set_ylim(0.0, 1.0)\n",
    "    ax.set_xlabel(\"FP rate\")\n",
    "    ax.set_ylabel(\"TP rate\")\n",
    "    ax.set_title(\"ROC chart for %s\" % target)\n",
    "\n",
    "    ax.plot(\n",
    "        [0, 1],\n",
    "        [0, 1],\n",
    "        color=\"navy\",\n",
    "        label=\"random\",\n",
    "        linewidth=1,\n",
    "        linestyle=\"--\",\n",
    "        marker=\"\",\n",
    "    )\n",
    "    models = list(predictions.keys())\n",
    "    for i in range(len(models)):\n",
    "        RocCurveDisplay.from_predictions(\n",
    "            y_true=tstY,\n",
    "            y_pred=predictions[models[i]],\n",
    "            name=models[i],\n",
    "            ax=ax,\n",
    "            color=ACTIVE_COLORS[i],\n",
    "            linewidth=1,\n",
    "        )\n",
    "    ax.legend(loc=\"lower right\", fontsize=\"xx-small\")\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_evaluation_results(model, trn_y, prd_trn, tst_y, prd_tst, labels: ndarray) -> ndarray:\n",
    "    evaluation: dict = {}\n",
    "    for key in CLASS_EVAL_METRICS:\n",
    "        evaluation[key] = [\n",
    "            CLASS_EVAL_METRICS[key](trn_y, prd_trn),\n",
    "            CLASS_EVAL_METRICS[key](tst_y, prd_tst),\n",
    "        ]\n",
    "\n",
    "    params_st: str = \"\" if () == model[\"params\"] else str(model[\"params\"])\n",
    "    fig: Figure\n",
    "    axs: ndarray\n",
    "    fig, axs = subplots(1, 2, figsize=(2 * HEIGHT, HEIGHT))\n",
    "    fig.suptitle(f'Best {model[\"metric\"]} for {model[\"name\"]} {params_st}')\n",
    "    plot_multibar_chart([\"Train\", \"Test\"], evaluation, ax=axs[0], percentage=True)\n",
    "\n",
    "    cnf_mtx_tst: ndarray = confusion_matrix(tst_y, prd_tst, labels=labels)\n",
    "    plot_confusion_matrix(cnf_mtx_tst, labels, ax=axs[1])\n",
    "    return axs\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "#             TIME SERIES\n",
    "# ---------------------------------------\n",
    "\n",
    "\n",
    "from statsmodels.tsa.seasonal import DecomposeResult, seasonal_decompose\n",
    "\n",
    "\n",
    "def plot_ts_multivariate_chart(data: DataFrame, title: str) -> list[Axes]:\n",
    "    fig: Figure\n",
    "    axs: list[Axes]\n",
    "    fig, axs = subplots(data.shape[1], 1, figsize=(3 * HEIGHT, HEIGHT / 2 * data.shape[1]))\n",
    "    fig.suptitle(title)\n",
    "\n",
    "    for i in range(data.shape[1]):\n",
    "        col: str = data.columns[i]\n",
    "        auxi_ax: Axes = plot_line_chart(\n",
    "            data[col].index.to_list(),\n",
    "            data[col].to_list(),\n",
    "            ax=axs[i],\n",
    "            xlabel=data.index.name,\n",
    "            ylabel=col,\n",
    "        )\n",
    "        auxi_ax.tick_params(axis=\"x\", labelbottom=\"off\")\n",
    "    return axs\n",
    "\n",
    "\n",
    "def plot_components(\n",
    "    series: Series,\n",
    "    title: str = \"\",\n",
    "    x_label: str = \"time\",\n",
    "    y_label: str = \"\",\n",
    ") -> list[Axes]:\n",
    "    decomposition: DecomposeResult = seasonal_decompose(series, model=\"add\")\n",
    "    components: dict = {\n",
    "        \"observed\": series,\n",
    "        \"trend\": decomposition.trend,\n",
    "        \"seasonal\": decomposition.seasonal,\n",
    "        \"residual\": decomposition.resid,\n",
    "    }\n",
    "    rows: int = len(components)\n",
    "    fig: Figure\n",
    "    axs: list[Axes]\n",
    "    fig, axs = subplots(rows, 1, figsize=(3 * HEIGHT, rows * HEIGHT))\n",
    "    fig.suptitle(f\"{title}\")\n",
    "    i: int = 0\n",
    "    for key in components:\n",
    "        set_chart_labels(axs[i], title=key, xlabel=x_label, ylabel=y_label)\n",
    "        axs[i].plot(components[key])\n",
    "        i += 1\n",
    "    return axs\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "#       TIME SERIES TRANSFORMATION\n",
    "# ---------------------------------------\n",
    "\n",
    "\n",
    "def ts_aggregation_by(\n",
    "    data: Series | DataFrame,\n",
    "    gran_level: str = \"D\",\n",
    "    agg_func: str = \"mean\",\n",
    ") -> Series | DataFrame:\n",
    "    df: Series | DataFrame = data.copy()\n",
    "    index: Index[Period] = df.index.to_period(gran_level)\n",
    "    df = df.groupby(by=index, dropna=True, sort=True).agg(agg_func)\n",
    "    df.index.drop_duplicates()\n",
    "    df.index = df.index.to_timestamp()\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def series_train_test_split(data: Series, trn_pct: float = 0.90) -> tuple[Series, Series]:\n",
    "    trn_size: int = int(len(data) * trn_pct)\n",
    "    df_cp: Series = data.copy()\n",
    "    train: Series = df_cp.iloc[:trn_size, 0]\n",
    "    test: Series = df_cp.iloc[trn_size:, 0]\n",
    "    return train, test\n",
    "\n",
    "def dataframe_temporal_train_test_split(data: DataFrame, trn_pct: float = 0.90) -> tuple[DataFrame, DataFrame]:\n",
    "    trn_size: int = int(len(data) * trn_pct)\n",
    "    df_cp: DataFrame = data.copy()\n",
    "    train: DataFrame = df_cp.iloc[:trn_size]\n",
    "    test: DataFrame = df_cp.iloc[trn_size:]\n",
    "    return train, test\n",
    "\n",
    "\n",
    "# ---------------------------------------\n",
    "#             FORECASTING\n",
    "# ---------------------------------------\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "\n",
    "\n",
    "FORECAST_MEASURES = {\n",
    "    \"MSE\": mean_squared_error,\n",
    "    \"MAE\": mean_absolute_error,\n",
    "    \"R2\": r2_score,\n",
    "    \"MAPE\": mean_absolute_percentage_error,\n",
    "}\n",
    "\n",
    "\n",
    "def plot_forecasting_series(\n",
    "    trn: Series,\n",
    "    tst: Series,\n",
    "    prd_tst: Series,\n",
    "    title: str = \"\",\n",
    "    xlabel: str = \"time\",\n",
    "    ylabel: str = \"\",\n",
    ") -> list[Axes]:\n",
    "    fig, ax = subplots(1, 1, figsize=(4 * HEIGHT, HEIGHT), squeeze=True)\n",
    "    fig.suptitle(title)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.plot(trn.index, trn.values, label=\"train\", color=PAST_COLOR)\n",
    "    ax.plot(tst.index, tst.values, label=\"test\", color=FUTURE_COLOR)\n",
    "    ax.plot(prd_tst.index, prd_tst.values, \"--\", label=\"test prediction\", color=PRED_FUTURE_COLOR)\n",
    "    ax.legend(prop={\"size\": 5})\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def plot_forecasting_eval(trn: Series, tst: Series, prd_trn: Series, prd_tst: Series, title: str = \"\") -> list[Axes]:\n",
    "    ev1: dict = {\n",
    "        \"RMSE\": [sqrt(FORECAST_MEASURES[\"MSE\"](trn, prd_trn)), sqrt(FORECAST_MEASURES[\"MSE\"](tst, prd_tst))],\n",
    "        \"MAE\": [FORECAST_MEASURES[\"MAE\"](trn, prd_trn), FORECAST_MEASURES[\"MAE\"](tst, prd_tst)],\n",
    "    }\n",
    "    ev2: dict = {\n",
    "        \"MAPE\": [FORECAST_MEASURES[\"MAPE\"](trn, prd_trn), FORECAST_MEASURES[\"MAPE\"](tst, prd_tst)],\n",
    "        \"R2\": [FORECAST_MEASURES[\"R2\"](trn, prd_trn), FORECAST_MEASURES[\"R2\"](tst, prd_tst)],\n",
    "    }\n",
    "\n",
    "    # print(eval1, eval2)\n",
    "    fig, axs = subplots(1, 2, figsize=(1.5 * HEIGHT, 0.75 * HEIGHT), squeeze=True)\n",
    "    fig.suptitle(title)\n",
    "    plot_multibar_chart([\"train\", \"test\"], ev1, ax=axs[0], title=\"Scale-dependent error\", percentage=False)\n",
    "    plot_multibar_chart([\"train\", \"test\"], ev2, ax=axs[1], title=\"Percentage error\", percentage=True)\n",
    "\n",
    "    return axs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Projeto/Forecasting/Arrests_testing_data_arrest_decision_tree.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m target \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCLASS\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     41\u001b[0m eval_metric \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 43\u001b[0m trnX, tstX, trnY, tstY, labels, \u001b[38;5;28mvars\u001b[39m \u001b[38;5;241m=\u001b[39m read_train_test_from_files(train_filename, test_filename, target)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain#=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(trnX)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Test#=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(tstX)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabels=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 672\u001b[0m, in \u001b[0;36mread_train_test_from_files\u001b[0;34m(train_fn, test_fn, target)\u001b[0m\n\u001b[1;32m    669\u001b[0m trnY: array \u001b[38;5;241m=\u001b[39m train\u001b[38;5;241m.\u001b[39mpop(target)\u001b[38;5;241m.\u001b[39mto_list()\n\u001b[1;32m    670\u001b[0m trnX: ndarray \u001b[38;5;241m=\u001b[39m train\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m--> 672\u001b[0m test: DataFrame \u001b[38;5;241m=\u001b[39m read_csv(test_fn, index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    673\u001b[0m tstY: array \u001b[38;5;241m=\u001b[39m test\u001b[38;5;241m.\u001b[39mpop(target)\u001b[38;5;241m.\u001b[39mto_list()\n\u001b[1;32m    674\u001b[0m tstX: ndarray \u001b[38;5;241m=\u001b[39m test\u001b[38;5;241m.\u001b[39mvalues\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Projeto/Forecasting/Arrests_testing_data_arrest_decision_tree.csv'"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "from numpy import array, ndarray\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from matplotlib.pyplot import figure, savefig, show\n",
    "\n",
    "\n",
    "def knn_study(\n",
    "        trnX: ndarray, trnY: array, tstX: ndarray, tstY: array, k_max: int=19, lag: int=2, metric='accuracy'\n",
    "        ) -> tuple[KNeighborsClassifier | None, dict]:\n",
    "    dist: list[Literal['manhattan', 'euclidean', 'chebyshev']] = ['manhattan', 'euclidean', 'chebyshev']\n",
    "\n",
    "    kvalues: list[int] = [i for i in range(1, k_max+1, lag)]\n",
    "    best_model: KNeighborsClassifier | None = None\n",
    "    best_params: dict = {'name': 'KNN', 'metric': metric, 'params': ()}\n",
    "    best_performance: float = 0.0\n",
    "\n",
    "    values: dict[str, list] = {}\n",
    "    for d in dist:\n",
    "        y_tst_values: list = []\n",
    "        for k in kvalues:\n",
    "            clf = KNeighborsClassifier(n_neighbors=k, metric=d)\n",
    "            clf.fit(trnX, trnY)\n",
    "            prdY: array = clf.predict(tstX)\n",
    "            eval: float = CLASS_EVAL_METRICS[metric](tstY, prdY)\n",
    "            y_tst_values.append(eval)\n",
    "            if eval - best_performance > DELTA_IMPROVE:\n",
    "                best_performance: float = eval\n",
    "                best_params['params'] = (k, d)\n",
    "                best_model = clf\n",
    "            # print(f'KNN {d} k={k}')\n",
    "        values[d] = y_tst_values\n",
    "    print(f'KNN best with k={best_params['params'][0]} and {best_params['params'][1]}')\n",
    "    plot_multiline_chart(kvalues, values, title=f'KNN Models ({metric})', xlabel='k', ylabel=metric, percentage=True)\n",
    "\n",
    "    return best_model, best_params\n",
    "\n",
    "file_tag = 'arrest'\n",
    "train_filename = '/Users/tomifemme/Desktop/DataScience/Projeto/Forecasting/Arrests_training_data_arrest_decision_tree.csv'\n",
    "test_filename = '/Users/tomifemme/Desktop/DataScience/Projeto/Forecasting/Arrests_testing_data_arrest_decision_tree.csv'\n",
    "target = 'CLASS'\n",
    "eval_metric = 'accuracy'\n",
    "\n",
    "trnX, tstX, trnY, tstY, labels, vars = read_train_test_from_files(train_filename, test_filename, target)\n",
    "print(f'Train#={len(trnX)} Test#={len(tstX)}')\n",
    "print(f'Labels={labels}')\n",
    "\n",
    "figure()\n",
    "best_model, params = knn_study(trnX, trnY, tstX, tstY, k_max=25, metric=eval_metric)\n",
    "savefig(f'images/{file_tag}_knn_{eval_metric}_study.png')\n",
    "show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
