{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "from sklearn.base import accuracy_score\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "\n",
    "CLASS_EVAL_METRICS: dict[str, Callable] = {\n",
    "    \"accuracy\": accuracy_score,\n",
    "    \"recall\": recall_score,\n",
    "    \"precision\": precision_score,\n",
    "    \"auc\": roc_auc_score,\n",
    "    \"f1\": f1_score,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DELTA_IMPROVE: float = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array, ndarray\n",
    "from pandas import DataFrame, read_csv\n",
    "\n",
    "\n",
    "def read_train_test_from_files(\n",
    "    train_fn: str, test_fn: str, target: str = \"class\"\n",
    ") -> tuple[ndarray, ndarray, array, array, list, list]:\n",
    "    train: DataFrame = read_csv(train_fn, index_col=None)\n",
    "    labels: list = list(train[target].unique())\n",
    "    labels.sort()\n",
    "    trnY: array = train.pop(target).to_list()\n",
    "    trnX: ndarray = train.values\n",
    "\n",
    "    test: DataFrame = read_csv(test_fn, index_col=None)\n",
    "    tstY: array = test.pop(target).to_list()\n",
    "    tstX: ndarray = test.values\n",
    "    return trnX, tstX, trnY, tstY, labels, train.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from matplotlib.axes import Axes\n",
    "from matplotlib.dates import AutoDateFormatter, AutoDateLocator\n",
    "def set_chart_xticks(xvalues: list[str | int | float | datetime], ax: Axes, percentage: bool = False) -> Axes:\n",
    "    if len(xvalues) > 0:\n",
    "        if percentage:\n",
    "            ax.set_ylim(0.0, 1.0)\n",
    "\n",
    "        if isinstance(xvalues[0], datetime):\n",
    "            locator = AutoDateLocator()\n",
    "            ax.xaxis.set_major_locator(locator)\n",
    "            ax.xaxis.set_major_formatter(AutoDateFormatter(locator, defaultfmt=\"%Y-%m-%d\"))\n",
    "        rotation: int = 0\n",
    "        if not any(not isinstance(x, (int, float)) for x in xvalues):\n",
    "            ax.set_xlim(left=xvalues[0], right=xvalues[-1])\n",
    "            ax.set_xticks(xvalues, labels=xvalues)\n",
    "        else:\n",
    "            rotation = 45\n",
    "\n",
    "        ax.tick_params(axis=\"x\", labelrotation=rotation, labelsize=\"xx-small\")\n",
    "\n",
    "    return ax\n",
    "\n",
    "def set_chart_labels(ax: Axes, title: str = \"\", xlabel: str = \"\", ylabel: str = \"\") -> Axes:\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    return ax\n",
    "\n",
    "def plot_multiline_chart(\n",
    "    xvalues: list,\n",
    "    yvalues: dict,\n",
    "    ax: Axes = None,  # type: ignore\n",
    "    title: str = \"\",\n",
    "    xlabel: str = \"\",\n",
    "    ylabel: str = \"\",\n",
    "    percentage: bool = False,\n",
    ") -> Axes:\n",
    "    if ax is None:\n",
    "        ax = gca()\n",
    "    ax = set_chart_labels(ax=ax, title=title, xlabel=xlabel, ylabel=ylabel)\n",
    "    ax = set_chart_xticks(xvalues, ax=ax, percentage=percentage)\n",
    "    legend: list = []\n",
    "    for name, y in yvalues.items():\n",
    "        ax.plot(xvalues, y)\n",
    "        legend.append(name)\n",
    "        if any(v < 0 for v in y) and percentage:\n",
    "            ax.set_ylim(-1.0, 1.0)\n",
    "    ax.legend(legend, fontsize=\"xx-small\")\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import set_printoptions\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cnf_matrix: ndarray, classes_names: ndarray, ax: Axes = None) -> Axes:  # type: ignore\n",
    "    if ax is None:\n",
    "        ax = gca()\n",
    "    title = \"Confusion matrix\"\n",
    "    set_printoptions(precision=2)\n",
    "    tick_marks: ndarray = arange(0, len(classes_names), 1)\n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel(\"True label\")\n",
    "    ax.set_xlabel(\"Predicted label\")\n",
    "    ax.set_xticks(tick_marks)\n",
    "    ax.set_yticks(tick_marks)\n",
    "    ax.set_xticklabels(classes_names)\n",
    "    ax.set_yticklabels(classes_names)\n",
    "    ax.imshow(cnf_matrix, interpolation=\"nearest\", cmap=cmap_blues)\n",
    "\n",
    "    for i, j in product(range(cnf_matrix.shape[0]), range(cnf_matrix.shape[1])):\n",
    "        ax.text(j, i, format(cnf_matrix[i, j], \"d\"), color=\"y\", horizontalalignment=\"center\")\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.figure import Figure\n",
    "from matplotlib.pyplot import subplots\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "HEIGHT: int = 4\n",
    "def plot_evaluation_results(model, trn_y, prd_trn, tst_y, prd_tst, labels: ndarray) -> ndarray:\n",
    "    evaluation: dict = {}\n",
    "    for key in CLASS_EVAL_METRICS:\n",
    "        evaluation[key] = [\n",
    "            CLASS_EVAL_METRICS[key](trn_y, prd_trn),\n",
    "            CLASS_EVAL_METRICS[key](tst_y, prd_tst),\n",
    "        ]\n",
    "\n",
    "    params_st: str = \"\" if () == model[\"params\"] else str(model[\"params\"])\n",
    "    fig: Figure\n",
    "    axs: ndarray\n",
    "    fig, axs = subplots(1, 2, figsize=(2 * HEIGHT, HEIGHT))\n",
    "    fig.suptitle(f'Best {model[\"metric\"]} for {model[\"name\"]} {params_st}')\n",
    "    plot_multibar_chart([\"Train\", \"Test\"], evaluation, ax=axs[0], percentage=True)\n",
    "\n",
    "    cnf_mtx_tst: ndarray = confusion_matrix(tst_y, prd_tst, labels=labels)\n",
    "    plot_confusion_matrix(cnf_mtx_tst, labels, ax=axs[1])\n",
    "    return axs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from numpy import array, ndarray\n",
    "from matplotlib.pyplot import subplots, figure, savefig, show\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from dslabs_functions import (\n",
    "    CLASS_EVAL_METRICS,\n",
    "    DELTA_IMPROVE,\n",
    "    read_train_test_from_files,\n",
    ")\n",
    "from dslabs_functions import HEIGHT, plot_evaluation_results, plot_multiline_chart\n",
    "\n",
    "LAG: int = 500\n",
    "NR_MAX_ITER: int = 5000\n",
    "\n",
    "\n",
    "def mlp_study(\n",
    "    trnX: ndarray,\n",
    "    trnY: array,\n",
    "    tstX: ndarray,\n",
    "    tstY: array,\n",
    "    nr_max_iterations: int = 2500,\n",
    "    lag: int = 500,\n",
    "    metric: str = \"accuracy\",\n",
    ") -> tuple[MLPClassifier | None, dict]:\n",
    "    nr_iterations: list[int] = [lag] + [\n",
    "        i for i in range(2 * lag, nr_max_iterations + 1, lag)\n",
    "    ]\n",
    "\n",
    "    lr_types: list[Literal[\"constant\", \"invscaling\", \"adaptive\"]] = [\n",
    "        \"constant\",\n",
    "        \"invscaling\",\n",
    "        \"adaptive\",\n",
    "    ]  # only used if optimizer='sgd'\n",
    "    learning_rates: list[float] = [0.5, 0.05, 0.005, 0.0005]\n",
    "\n",
    "    best_model: MLPClassifier | None = None\n",
    "    best_params: dict = {\"name\": \"MLP\", \"metric\": metric, \"params\": ()}\n",
    "    best_performance: float = 0.0\n",
    "\n",
    "    values: dict = {}\n",
    "    _, axs = subplots(\n",
    "        1, len(lr_types), figsize=(len(lr_types) * HEIGHT, HEIGHT), squeeze=False\n",
    "    )\n",
    "    for i in range(len(lr_types)):\n",
    "        type: str = lr_types[i]\n",
    "        values = {}\n",
    "        for lr in learning_rates:\n",
    "            warm_start: bool = False\n",
    "            y_tst_values: list[float] = []\n",
    "            for j in range(len(nr_iterations)):\n",
    "                clf = MLPClassifier(\n",
    "                    learning_rate=type,\n",
    "                    learning_rate_init=lr,\n",
    "                    max_iter=lag,\n",
    "                    warm_start=warm_start,\n",
    "                    activation=\"logistic\",\n",
    "                    solver=\"sgd\",\n",
    "                    verbose=False,\n",
    "                )\n",
    "                clf.fit(trnX, trnY)\n",
    "                prdY: array = clf.predict(tstX)\n",
    "                eval: float = CLASS_EVAL_METRICS[metric](tstY, prdY)\n",
    "                y_tst_values.append(eval)\n",
    "                warm_start = True\n",
    "                if eval - best_performance > DELTA_IMPROVE:\n",
    "                    best_performance = eval\n",
    "                    best_params[\"params\"] = (type, lr, nr_iterations[j])\n",
    "                    best_model = clf\n",
    "                # print(f'MLP lr_type={type} lr={lr} n={nr_iterations[j]}')\n",
    "            values[lr] = y_tst_values\n",
    "        plot_multiline_chart(\n",
    "            nr_iterations,\n",
    "            values,\n",
    "            ax=axs[0, i],\n",
    "            title=f\"MLP with {type}\",\n",
    "            xlabel=\"nr iterations\",\n",
    "            ylabel=metric,\n",
    "            percentage=True,\n",
    "        )\n",
    "    print(\n",
    "        f'MLP best for {best_params[\"params\"][2]} iterations (lr_type={best_params[\"params\"][0]} and lr={best_params[\"params\"][1]}'\n",
    "    )\n",
    "\n",
    "    return best_model, best_params\n",
    "\n",
    "\n",
    "file_tag = \"stroke\"\n",
    "train_filename = \"data/stroke_train_smote.csv\"\n",
    "test_filename = \"data/stroke_test.csv\"\n",
    "target = \"stroke\"\n",
    "eval_metric = \"accuracy\"\n",
    "\n",
    "trnX, tstX, trnY, tstY, labels, vars = read_train_test_from_files(\n",
    "    train_filename, test_filename, target\n",
    ")\n",
    "print(f\"Train#={len(trnX)} Test#={len(tstX)}\")\n",
    "print(f\"Labels={labels}\")\n",
    "\n",
    "figure()\n",
    "best_model, params = mlp_study(\n",
    "    trnX,\n",
    "    trnY,\n",
    "    tstX,\n",
    "    tstY,\n",
    "    nr_max_iterations=NR_MAX_ITER,\n",
    "    lag=LAG,\n",
    "    metric=eval_metric,\n",
    ")\n",
    "savefig(f\"images/{file_tag}_mlp_{eval_metric}_study.png\")\n",
    "show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
